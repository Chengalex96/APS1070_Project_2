{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Project_2_w21.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ucvm7IH6da3"
      },
      "source": [
        "# **Project 2**, APS1070 Winter 2021\n",
        "**Anomaly Detection Algorithm using Gaussian Mixture Model [15 Marks]**\n",
        "\n",
        "**Deadline: Feb 27, 11 PM - 15 points**\n",
        "\n",
        "**Academic Integrity**\n",
        "\n",
        "This project is individual - it is to be completed on your own. If you have questions, please post your query in the APS1070 Piazza Q&A forums (the answer might be useful to others!).\n",
        "\n",
        "Do not share your code with others, or post your work online. Do not submit code that you have not written yourself. Students suspected of plagiarism on a project, midterm or exam will be referred to the department for formal discipline for breaches of the Student Code of Conduct."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGTYOwVXnmGv"
      },
      "source": [
        "Please fill out the following:\n",
        "\n",
        "\n",
        "*   Name: \n",
        "*   Student Number: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVh8MGvte8bX"
      },
      "source": [
        "##**Part 1: Getting started [2 Marks]**\n",
        "\n",
        "We are going to work with a credit card fraud dataset. This dataset contains 28 key features, which are not \n",
        "directly interpretable but contain meaningful information about the dataset.\n",
        "\n",
        "Load the dataset in CSV file using Pandas. The dataset is called `creditcard.csv`. Print out the first few columns of the dataset.\n",
        "\n",
        "* How many rows are there? _____ **[0.1]**\n",
        "* What features in the dataset are present aside from the 28 main features?  _____ **[0.1]**\n",
        "* Which column contains the targets? **[0.1]**\n",
        "* To what do the target values correspond?_____ **[0.1]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9LfYqXUHbql"
      },
      "source": [
        "pip install wget"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sg0gndnDe8bd"
      },
      "source": [
        "import wget\n",
        "\n",
        "wget.download('https://github.com/aps1070-2019/datasets/raw/master/creditcard.tar.gz','creditcard.tar.gz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h5z71s8e8bm"
      },
      "source": [
        "!tar -zxvf creditcard.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bojUxOaHW5si"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "df = pd.read_csv('creditcard.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xR6EPYQZVoa"
      },
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1w6cRXOee8b3"
      },
      "source": [
        "It's important when looking at a new dataset to figure out how many examples we have for each class.\n",
        "\n",
        "* What is the percentage of entries in the dataset for each class? _____ **[0.1]**\n",
        "* Is this data considered balanced or unbalanced? Why is this the case?_____ **[0.1]**\n",
        "* Why is balance/imbalance important? How might this class ditribution affect a KNN classifier for example, which we explored in Project 1? _____ **[0.2]**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2kHRnhzZUTk"
      },
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4EPw3I-e8b7"
      },
      "source": [
        "Next, split the dataset into a training (70%) and testing set (30%). Set the random state to 0.**[0.2]**\n",
        "\n",
        "Make sure to separate out the column corresponding to the targets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZAbDaphe8bt"
      },
      "source": [
        "### Split the data  ###\n",
        "X_train, X_test, y_train, y_test = 0, 0, 0, 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe41hNLJe8cQ"
      },
      "source": [
        "Now, let's take a look at the difference in distribution for some variables between fraudulent and non-fraudulent transactions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "YTGw4xNde8cV"
      },
      "source": [
        "features=[f for f in df.columns if 'V' in f]\n",
        "nplots=np.size(features)\n",
        "plt.figure(figsize=(15,4*nplots))\n",
        "gs = gridspec.GridSpec(nplots,1)\n",
        "for i, feat in enumerate(features):\n",
        "    ax = plt.subplot(gs[i])\n",
        "    sns.histplot(X_train[feat][y_train==1], stat=\"density\", kde=True, color=\"blue\", bins=50)\n",
        "    sns.histplot(X_train[feat][y_train==0], stat=\"density\", kde=True, color=\"red\", bins=50)\n",
        "    ax.legend(['fraudulent', 'non-fraudulent'],loc='best')\n",
        "    ax.set_xlabel('')\n",
        "    ax.set_title('Distribution of feature: ' + feat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCCCEVDHeC2P"
      },
      "source": [
        "Explain how these graphs could provide meaningful information about anomaly detection using a gaussian model. **[1]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eomFV8FyIr41"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eecAuMsDqQaC"
      },
      "source": [
        "## **Part 2: Single feature model with one Gaussian distribution: [2.5 Marks]**\n",
        "We'll start by making a prediction using **a single feature of our dataset at a time**. \n",
        "\n",
        "  1. Fit a single Gaussian distribution on a single feature of **the full training dataset** (both classes) using ``sklearn.mixture.GaussianMixture`` when ``n_components=1``. **[0.25]**\n",
        "  2. Compute AUC (Area under the ROC Curve) based on ``sklearn.mixture.GaussianMixture.score_samples`` on the full training set (including both classes).  **[0.25]**\n",
        "  3. Repeat the above steps for each of the features and present your findings in a table. **[0.5]**\n",
        "  4. Find the best feature to distinguish fraudulent transactions from non-fraudulent transactions based on AUC. **[0.25]**\n",
        "  5. Make a prediction based on a model's scores: If the `score_samples` is lower than a threshold, we consider that transaction as a fraud. Find an optimal threshold that maximizes the F1 Score in the training set. (Do not check every possible value for threshold, come up with a faster way!) Compute F1 score using `sklearn.metrics.f1_score`. **[0.5]**\n",
        "\n",
        "  6. If we fit our Gaussian on only non-fraudulent transactions instead of the whole training set (in step 1), how would that change our model's performance? why? Experiment and explain! **[0.75]**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Vbf34-zsbMa"
      },
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViJHrg4AfICo"
      },
      "source": [
        "## **Part 3: Multiple feature model with one Gaussian distribution: [2.5 Marks]**\n",
        "This part is similar to Part 2, but here we will pick two features and set the number of components visually.\n",
        "\n",
        " 1. Pick two features (say, f1 and f2). \n",
        " 2. Scatter plot (plt.scatter) those features on a figure (f1 on the x-axis and f2 on the y-axis). **[0.25]**\n",
        " 3. On the scatter plot color the data-points based on their class (non-fraudulents blue and fraudulents red). **[0.25]**\n",
        " 4. Based on your plots decide how many Gaussian components (``n_components``) you need to fit the data (focus on valid transactions). Explain.  **[0.5]**\n",
        " 5. Fit your Gaussian model on all the data-points. **[0.25]**\n",
        " 6. Compute AUC **[0.25]**\n",
        " 7. Pick 3 new sets of features and repeat steps 2 to 6. **[0.5]**\n",
        " 8. Pick the set with the highest AUC.\n",
        " 9. Find a threshold to maximize your F1 Score.**[0.25]**\n",
        " 10. Plot a figure similar to step 3 and put a circle around outliers based on your threshold (use the code of the similar figure in tutorial) **[0.25]**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtcOHF9bnYY_"
      },
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEJUxd-6inbh"
      },
      "source": [
        "## **Part 4: Single feature model with two Gaussian distributions. [3 Marks]**\n",
        "Now we will use two separate distributions for fraudulent and non-fraudulent transactions.\n",
        "  1.  Fit a Gaussian distribution ($G_1$) on a feature of **non-fraudulent transactions** using ``sklearn.mixture.GaussianMixture`` when ``n_components=1``. **[0.5]**\n",
        "  2. Fit another Gaussian distribution ($G_2$) on the same feature but for **fraudulent transactions** using ``sklearn.mixture.GaussianMixture`` when ``n_components=1``. **[0.5]**\n",
        "  3. Compute the score samples ($S$) for both $G_1$ and $G_2$ on the **whole training set** to get $S_1$ and $S_2$, respectively. **[0.5]**\n",
        "  4. Find an optimal $c$ (a real number) that maximizes F1 Score for a model such that if $S_1 < c \\times S_2$, the transaction is classified as a fraud. For example, if $c=1$ we could say that if $S_2$ is greater than $S_1$, ($S_1$<$S_2$) then the transaction is a fraud (the transaction belongs to the $G_2$ distribution which represents fraudulent transactions). For start consider $c$ in $[0,10]$ with steps of 0.1, you can change this window in your experiments if needed. **[0.5]**\n",
        "  5. Repeat the steps above for all the features. What is the best F1 Score that you get? Which feature and what c? **[1]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJpFiTj5jaAD"
      },
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac1nyvCPe8ce"
      },
      "source": [
        "## **Part 5: Multivariate and Mixture of Gaussians Distribution [4 Marks]**\n",
        "We now want to build an outlier detection model that performs well in terms of F1 score. To design your model, you can benefit from:\n",
        "\n",
        "*   No restrictions on the number of features - use as few or as many as you want! (multivariate). \n",
        "*   To fit your model, you can take advantage of the Gaussian mixture model where you can set the number of components [help](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html) .\n",
        "*   You can choose to fit your Gaussians on non-fraudulent transactions or to both classes. \n",
        "\n",
        "\n",
        "It is up to you how to design your model. Try at least 10 different models and report the AUC (if applicable) and the best F1 score for each one. What kind of model works better? How many features are best (and which ones)? How many Gaussians? How many components? Summarize your findings with tables and plots. **[4]**\n",
        "\n",
        "\n",
        "**HINT !**\n",
        "\n",
        "Want an F1 score above $85\\%$? Try a two gaussian model, multiple features, single component for valid transaction and multiple components for fraudulent ones! Why does it make sense to have multiple components for the fraudulent transactions?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vN1H_N-TJZ0H"
      },
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NzRkO9sIwKS"
      },
      "source": [
        "## **Part 6: Evaluating performance on test set: [1 Mark]**\n",
        "**Which model worked better?** Pick your best model among all models and apply it to your test set. Report the F1 Score, precision and recall on the test set. **[1]**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yClYMXloe8cg"
      },
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}